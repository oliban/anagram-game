# CI/CD Test Runner Execution Guide

## When Does the Test Runner Execute?

The automated test runner is designed to run at **multiple strategic points** in the development lifecycle to ensure code quality and catch issues early.

## ğŸ”„ Execution Triggers

### 1. **Code Changes (Most Common)**

#### **Every Push to Main/Develop Branches**
```yaml
on:
  push:
    branches: [ main, develop ]
    paths:
      - 'services/**'
      - 'testing/**'
      - 'docker-compose.services.yml'
```

**What happens:**
- âœ… Full API test suite (5-10 minutes)
- âš ï¸ Performance tests (only on main branch)
- ğŸ“Š Comprehensive reporting
- ğŸš¨ Deployment gate evaluation

**Use case:** Validate that new code doesn't break existing functionality

#### **Every Pull Request**
```yaml
on:
  pull_request:
    branches: [ main ]
```

**What happens:**
- âœ… Full API test suite (5-10 minutes)
- âŒ Skip performance tests (faster feedback)
- ğŸ’¬ Comment results on PR
- ğŸš« Block merge if critical tests fail

**Use case:** Prevent broken code from being merged

### 2. **Scheduled Runs (Monitoring)**

#### **Nightly Health Checks**
```yaml
schedule:
  - cron: '0 2 * * *'  # 2 AM UTC daily
```

**What happens:**
- âœ… Full test suite including performance
- ğŸ” Memory leak detection
- ğŸ“ˆ Performance baseline validation
- ğŸš¨ Create GitHub issue if failures detected

**Use case:** Detect infrastructure drift or gradual degradation

#### **Business Hours Monitoring**
```yaml
schedule:
  - cron: '0 14 * * 1-5'  # 2 PM UTC weekdays
```

**What happens:**
- âœ… Critical tests only (faster)
- ğŸ“Š Service health validation
- ğŸ”” Immediate notifications if issues found

**Use case:** Early warning system during active development hours

### 3. **Manual Triggers (On-Demand)**

#### **Workflow Dispatch (GitHub Actions)**
```yaml
workflow_dispatch:
  inputs:
    skip_performance:
      description: 'Skip performance tests'
      default: 'true'
```

**What happens:**
- âœ… Configurable test execution
- ğŸ¯ Custom parameters (skip performance, stop on failure)
- ğŸ“‹ Full reporting for investigation

**Use case:** Debug issues, validate fixes, pre-deployment checks

#### **Jenkins Manual Builds**
- **Build Now** button in Jenkins UI
- **Parameterized builds** with custom options
- **Remote API triggers** from external tools

### 4. **Deployment Pipeline Integration**

#### **Pre-Staging Deployment**
```groovy
stage('ğŸš€ Deployment Gate') {
    when { branch 'main' }
    steps {
        // Evaluate test results
        // Block deployment if success rate < 95%
    }
}
```

**What happens:**
- âœ… **95%+ success rate** â†’ âœ… Deployment approved
- âš ï¸ **90-95% success rate** â†’ âš ï¸ Manual approval required  
- âŒ **<90% success rate** â†’ âŒ Deployment blocked

#### **Post-Deployment Validation**
```bash
# After deployment to staging/production
curl -X POST "https://jenkins.example.com/job/api-tests/buildWithParameters" \
  -d "ENVIRONMENT=staging&NOTIFY_ON_FAILURE=true"
```

**Use case:** Validate that deployed services are working correctly

## ğŸ¯ Execution Contexts

### **Development Context (Feature Branches)**
```
Trigger: Push to feature/branch
Duration: ~5-8 minutes
Tests: API + Integration + Workflows
Skip: Performance tests
Notify: Developer only
```

### **Integration Context (Main/Develop)**
```
Trigger: Push to main/develop
Duration: ~15-20 minutes  
Tests: All tests including performance
Skip: None
Notify: Team channels
Gate: Deployment approval
```

### **Monitoring Context (Scheduled)**
```
Trigger: Cron schedule
Duration: ~10-25 minutes
Tests: Full suite + health monitoring
Skip: None (comprehensive)
Notify: Operations team
Action: Create incident tickets
```

### **Emergency Context (Manual)**
```
Trigger: Manual/API call
Duration: Configurable
Tests: Targeted or full suite
Skip: Configurable
Notify: Requester
Purpose: Investigation/validation
```

## ğŸ“Š Real-World Execution Examples

### **Typical Day in Development:**

**9:00 AM** - Developer pushes feature branch
```
âœ… API Tests: 95% success (8 minutes)
ğŸ’¬ PR comment: "Tests mostly passing, 1 validation issue"
```

**10:30 AM** - Code review completed, PR merged to main
```
âœ… Full Test Suite: 98% success (18 minutes)  
âœ… Deployment Gate: APPROVED
ğŸš€ Auto-deploy to staging initiated
```

**2:00 PM** - Scheduled business hours check
```
âœ… Health Check: All services healthy (3 minutes)
ğŸ“Š Response times within normal range
```

**2:00 AM** - Nightly comprehensive check
```
âœ… Full Suite: 97% success (22 minutes)
ğŸ“ˆ Performance: Response times +5% (within tolerance)
ğŸ” Memory usage: Normal, no leaks detected
```

### **Issue Detection Scenario:**

**Tuesday 2:15 AM** - Nightly build detects issues
```
âŒ API Tests: 78% success rate
âŒ Socket.IO tests: Connection timeouts
ğŸš¨ GitHub issue auto-created: "Nightly API Tests Failed"
ğŸ“§ Email alert sent to on-call team
```

**Tuesday 8:30 AM** - Team investigates
```
ğŸ” Manual test run: SKIP_PERFORMANCE=true node automated-test-runner.js
ğŸ“‹ Reports show database connection issues
ğŸ”§ Infrastructure team notified
```

**Tuesday 9:15 AM** - Fix deployed, validation run
```
âœ… Manual test run: 99% success rate
âœ… Issue auto-closed by next scheduled run
ğŸ“Š Post-incident report generated
```

## ğŸ› ï¸ Configuration by Environment

### **Local Development**
```bash
# Quick validation before pushing
node testing/scripts/automated-test-runner.js --skip-performance
```

### **CI/CD Development Environment**
```yaml
env:
  SKIP_PERFORMANCE: "true"
  STOP_ON_FAILURE: "false"
  GENERATE_REPORTS: "true"
```

### **CI/CD Staging Environment**  
```yaml
env:
  SKIP_PERFORMANCE: "false"
  STOP_ON_FAILURE: "true"
  DEPLOYMENT_GATE: "true"
```

### **CI/CD Production Monitoring**
```yaml
env:
  SKIP_PERFORMANCE: "false"
  COMPREHENSIVE_LOGGING: "true"
  ALERT_THRESHOLDS: "strict"
```

## ğŸ“‹ Test Execution Matrix

| Trigger | API Tests | Performance | Workflows | Duration | Notifications |
|---------|-----------|-------------|-----------|----------|---------------|
| Feature Push | âœ… | âŒ | âœ… | 5-8 min | Developer |
| Main Push | âœ… | âœ… | âœ… | 15-20 min | Team |
| Pull Request | âœ… | âŒ | âœ… | 5-8 min | PR Comments |
| Nightly | âœ… | âœ… | âœ… | 20-25 min | Operations |
| Manual | ğŸ¯ Config | ğŸ¯ Config | ğŸ¯ Config | Variable | Requester |
| Pre-Deploy | âœ… | âœ… | âœ… | 15-20 min | Release Team |
| Post-Deploy | âœ… | âŒ | âœ… | 5-10 min | Ops + DevOps |

## ğŸš¨ Failure Response Workflows

### **Critical Failure (< 90% success)**
1. ğŸš« Block deployments immediately
2. ğŸš¨ Page on-call engineer  
3. ğŸ“‹ Auto-create incident ticket
4. ğŸ“§ Notify team leads
5. ğŸ”„ Prevent further merges to main

### **Warning Failure (90-95% success)**
1. âš ï¸ Mark build as unstable
2. ğŸ“§ Email development team
3. ğŸ“Š Generate detailed analysis report
4. ğŸ¤” Require manual deployment approval

### **Performance Degradation**
1. ğŸ“ˆ Compare against baseline metrics
2. ğŸ”” Notify performance team
3. ğŸ“Š Generate performance trending report
4. âš¡ Trigger infrastructure health checks

## ğŸ¯ Key Success Metrics

- **Response Time**: < 2 minutes for critical failures
- **Coverage**: 95%+ of API functionality tested
- **Reliability**: < 1% false positive rate  
- **Performance**: Catch >5% performance regressions
- **Integration**: Block 99%+ of breaking changes

The automated test runner serves as the **quality gateway** for the entire Wordshelf platform, ensuring that every code change is validated before reaching users.