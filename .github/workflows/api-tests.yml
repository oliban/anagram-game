# GitHub Actions Workflow for API Testing
# This workflow runs the automated test suite on various triggers

name: ðŸ§ª API Tests

on:
  # Trigger on every push to main branch
  push:
    branches: [ main, develop ]
    paths:
      - 'services/**'
      - 'testing/**'
      - 'docker-compose.services.yml'
      - '.github/workflows/api-tests.yml'

  # Trigger on all pull requests
  pull_request:
    branches: [ main ]
    paths:
      - 'services/**'
      - 'testing/**'
      - 'docker-compose.services.yml'

  # Manual trigger (workflow_dispatch)
  workflow_dispatch:
    inputs:
      skip_performance:
        description: 'Skip performance tests'
        required: false
        default: 'true'
        type: boolean
      stop_on_failure:
        description: 'Stop on first critical failure'
        required: false
        default: 'false'
        type: boolean

  # Scheduled runs (nightly builds)
  schedule:
    # Run at 2 AM UTC every day
    - cron: '0 2 * * *'
    # Run at 2 PM UTC on weekdays (business hours check)
    - cron: '0 14 * * 1-5'

env:
  NODE_VERSION: '18'
  API_URL: 'http://localhost:3000'

jobs:
  # Pre-flight checks
  preflight:
    name: ðŸ” Preflight Checks
    runs-on: ubuntu-latest
    outputs:
      should_run_performance: ${{ steps.check_context.outputs.run_performance }}
      test_environment: ${{ steps.check_context.outputs.environment }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Determine test context
        id: check_context
        run: |
          # Determine if we should run performance tests
          if [[ "${{ github.event_name }}" == "schedule" ]] || [[ "${{ github.ref }}" == "refs/heads/main" ]]; then
            echo "run_performance=true" >> $GITHUB_OUTPUT
            echo "environment=production" >> $GITHUB_OUTPUT
          elif [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
            echo "run_performance=${{ !inputs.skip_performance }}" >> $GITHUB_OUTPUT
            echo "environment=manual" >> $GITHUB_OUTPUT
          else
            echo "run_performance=false" >> $GITHUB_OUTPUT
            echo "environment=development" >> $GITHUB_OUTPUT
          fi
          
          echo "Test context: ${{ github.event_name }}"
          echo "Branch: ${{ github.ref }}"

  # Core API testing (always runs)
  api-tests:
    name: ðŸš€ API Tests
    runs-on: ubuntu-latest
    needs: preflight
    
    services:
      postgres:
        image: postgres:15-alpine
        env:
          POSTGRES_DB: anagram_game
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres123
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: |
          npm install
          npm install socket.io-client

      - name: Setup test environment
        run: |
          echo "Setting up test environment for: ${{ needs.preflight.outputs.test_environment }}"
          
          # Create necessary directories
          mkdir -p testing/reports
          
          # Set environment variables for tests
          echo "API_URL=${{ env.API_URL }}" >> $GITHUB_ENV
          echo "NODE_ENV=test" >> $GITHUB_ENV
          
          # Skip performance tests unless specifically enabled
          if [[ "${{ needs.preflight.outputs.should_run_performance }}" != "true" ]]; then
            echo "SKIP_PERFORMANCE=true" >> $GITHUB_ENV
          fi

      - name: Start services
        run: |
          echo "Starting Docker services..."
          docker-compose -f docker-compose.services.yml up -d --wait --wait-timeout 60
          
          # Wait for services to be fully ready
          sleep 10
          
          # Health check all services
          curl -f http://localhost:3000/api/status || exit 1
          curl -f http://localhost:3001/api/status || exit 1
          curl -f http://localhost:3002/api/status || exit 1
          curl -f http://localhost:3003/api/status || exit 1
          
          echo "All services are healthy âœ…"

      - name: Run automated test suite
        id: run_tests
        run: |
          echo "Running automated test suite..."
          
          # Set additional flags based on context
          FLAGS=""
          if [[ "${{ inputs.stop_on_failure }}" == "true" ]]; then
            FLAGS="$FLAGS --stop-on-failure"
          fi
          
          # Run the test suite
          node testing/scripts/automated-test-runner.js $FLAGS
        continue-on-error: true

      - name: Parse test results
        if: always()
        id: parse_results
        run: |
          # Extract key metrics from the latest test report
          if [[ -f "testing/reports/latest-test-summary.md" ]]; then
            OVERALL_SUCCESS=$(grep "Overall Success Rate" testing/reports/latest-test-summary.md | grep -o '[0-9.]*%' || echo "0%")
            PASSED_SUITES=$(grep "Test Suites:" testing/reports/latest-test-summary.md | grep -o '[0-9]*/[0-9]*' || echo "0/0")
            
            echo "overall_success_rate=$OVERALL_SUCCESS" >> $GITHUB_OUTPUT
            echo "passed_suites=$PASSED_SUITES" >> $GITHUB_OUTPUT
            
            echo "ðŸ“Š Test Results:"
            echo "Overall Success Rate: $OVERALL_SUCCESS"
            echo "Passed Suites: $PASSED_SUITES"
          fi

      - name: Upload test reports
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-reports-${{ github.run_number }}
          path: |
            testing/reports/
            testing/docs/TESTING_AUDIT_REPORT.md
          retention-days: 30

      - name: Create test summary comment (PR only)
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            let summaryContent = '## ðŸ§ª API Test Results\n\n';
            
            if (fs.existsSync('testing/reports/latest-test-summary.md')) {
              const content = fs.readFileSync('testing/reports/latest-test-summary.md', 'utf8');
              summaryContent += content;
            } else {
              summaryContent += 'âŒ Test reports not found. Check the workflow logs for details.\n';
            }
            
            summaryContent += `\n---\nðŸ“ [View detailed logs](${context.payload.repository.html_url}/actions/runs/${context.runId})`;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: summaryContent
            });

      - name: Check test results and fail if needed
        if: steps.run_tests.outcome == 'failure'
        run: |
          echo "âŒ Test suite failed with critical issues"
          echo "Overall Success Rate: ${{ steps.parse_results.outputs.overall_success_rate }}"
          echo "Passed Suites: ${{ steps.parse_results.outputs.passed_suites }}"
          exit 1

  # Performance testing (only on specific triggers)
  performance-tests:
    name: ðŸŽï¸ Performance Tests
    runs-on: ubuntu-latest
    needs: [preflight, api-tests]
    if: needs.preflight.outputs.should_run_performance == 'true'
    
    services:
      postgres:
        image: postgres:15-alpine
        env:
          POSTGRES_DB: anagram_game
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres123
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm install

      - name: Start services
        run: |
          docker-compose -f docker-compose.services.yml up -d --wait
          sleep 15  # Extra time for performance testing setup

      - name: Run performance tests
        run: |
          echo "ðŸŽï¸ Running performance test suite..."
          node testing/performance/test_performance_suite.js
        timeout-minutes: 15

      - name: Upload performance reports
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: performance-reports-${{ github.run_number }}
          path: testing/reports/
          retention-days: 30

  # Notification job (runs after all tests complete)
  notify:
    name: ðŸ“¢ Notify Results
    runs-on: ubuntu-latest
    needs: [api-tests, performance-tests]
    if: always() && (github.event_name == 'schedule' || github.ref == 'refs/heads/main')
    
    steps:
      - name: Determine overall status
        id: status
        run: |
          API_STATUS="${{ needs.api-tests.result }}"
          PERF_STATUS="${{ needs.performance-tests.result }}"
          
          if [[ "$API_STATUS" == "success" ]] && [[ "$PERF_STATUS" == "success" || "$PERF_STATUS" == "skipped" ]]; then
            echo "overall_status=success" >> $GITHUB_OUTPUT
            echo "status_emoji=âœ…" >> $GITHUB_OUTPUT
            echo "status_message=All tests passed successfully" >> $GITHUB_OUTPUT
          else
            echo "overall_status=failure" >> $GITHUB_OUTPUT
            echo "status_emoji=âŒ" >> $GITHUB_OUTPUT
            echo "status_message=Some tests failed - check the reports" >> $GITHUB_OUTPUT
          fi

      - name: Create GitHub issue on failure
        if: steps.status.outputs.overall_status == 'failure' && github.event_name == 'schedule'
        uses: actions/github-script@v7
        with:
          script: |
            github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: `ðŸš¨ Nightly API Tests Failed - ${new Date().toISOString().split('T')[0]}`,
              body: `## Automated Test Failure Report
              
              The nightly API test run has detected failures that require attention.
              
              **Run Details:**
              - Workflow: ${{ github.workflow }}
              - Run ID: ${{ github.run_id }}
              - Timestamp: ${new Date().toISOString()}
              - Branch: ${{ github.ref }}
              
              **Quick Links:**
              - [View Workflow Run](${context.payload.repository.html_url}/actions/runs/${context.runId})
              - [Download Test Reports](${context.payload.repository.html_url}/actions/runs/${context.runId}/artifacts)
              
              **Next Steps:**
              1. Review the test reports for specific failure details
              2. Check service logs for any infrastructure issues
              3. Run tests locally to reproduce the issues
              4. Create specific issue tickets for any bugs found
              
              This issue will be automatically closed when tests pass again.`,
              labels: ['bug', 'testing', 'automated']
            });

      # In a real setup, you might also send Slack notifications, emails, etc.
      - name: Log notification
        run: |
          echo "${{ steps.status.outputs.status_emoji }} ${{ steps.status.outputs.status_message }}"
          echo "Trigger: ${{ github.event_name }}"
          echo "Branch: ${{ github.ref }}"